{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Free Memory: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "781a0cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raw/Desktop/Coding/udit_new_military_int_icc/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/raw/Desktop/Coding/udit_new_military_int_icc/venv/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:520: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "HyperChakravyuhaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from PIL import Image\n",
    "import importlib.util\n",
    "import sys\n",
    "import time  # Import time module to introduce delay\n",
    "\n",
    "model_dir = '/home/raw/Desktop/Coding/military_int_icc/shakti-2B-041224'\n",
    "sys.path.append(model_dir)\n",
    "\n",
    "# Dynamically import the configuration and model\n",
    "config_module_path = f\"{model_dir}/configuration_shakti.py\"\n",
    "model_module_path = f\"{model_dir}/modeling_shakti.py\"\n",
    "\n",
    "# Load the configuration module dynamically\n",
    "spec_config = importlib.util.spec_from_file_location(\"shaktiConfig\", config_module_path)\n",
    "config_module = importlib.util.module_from_spec(spec_config)\n",
    "sys.modules[\"shaktiConfig\"] = config_module\n",
    "spec_config.loader.exec_module(config_module)\n",
    "\n",
    "# Load the model module dynamically\n",
    "spec_model = importlib.util.spec_from_file_location(\"shaktiModel\", model_module_path)\n",
    "model_module = importlib.util.module_from_spec(spec_model)\n",
    "sys.modules[\"shaktiModel\"] = model_module\n",
    "spec_model.loader.exec_module(model_module)\n",
    "\n",
    "# Now you can use the classes from the dynamically loaded modules\n",
    "from shaktiConfig import shaktiConfig\n",
    "from shaktiModel import shaktiModel\n",
    "\n",
    "# Load the custom model configuration\n",
    "config = shaktiConfig.from_pretrained(model_dir)\n",
    "\n",
    "# Load the custom model using the configuration\n",
    "model = shaktiModel.from_pretrained(model_dir, config=config, attn_implementation='sdpa', torch_dtype=torch.half)\n",
    "model.eval().cuda()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Initialize the processor (if custom processor function is provided in the model)\n",
    "processor = model.init_processor(tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23bf176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: \n",
      "Give me complete corrected lines using below context:\n",
      "Detected Text: State the universal law ak gravilalion. - Confidence: 0.9311787486076355\n",
      "Detected Text: a force called he gravilalional korce. the korce acling - Confidence: 0.8632688522338867\n",
      "Detected Text: Whai do you mean by free fall? - Confidence: 0.9386471509933472\n",
      "Detected Text: tonce. Hhe maion at dhe object is said lo have free fall. - Confidence: 0.8472165465354919\n",
      "Detected Text: What do youmean by accelenalion due lo giavidu? - Confidence: 0.8162661790847778\n",
      "Detected Text: When an object kalls'Howands the graund Knom a height, - Confidence: 0.8196004033088684\n",
      "Detected Text: Velocity praduces accelenalion in ihe obiect yhis - Confidence: 0.859413206577301\n",
      "Detected Text: acceleralian in Known ar acceleralion due to gravly.Ai - Confidence: 0.8293883800506592\n",
      "Detected Text: value is gwven by g.8 ml - Confidence: 0.8043153285980225\n",
      "Detected Text: Ik the moon allnaci the eailh;why does the failh - Confidence: 0.8403022289276123\n",
      "Detected Text: nat mave dowatds Jhe moon ? - Confidence: 0.8502532839775085\n",
      "Detected Text: yhe Eaih and the moon expencences equlal gravitalionad - Confidence: 0.8557673096656799\n",
      "Detected Text: tonce txom each othen.However, the may of The Eailh - Confidence: 0.8026825189590454\n",
      "Detected Text: W much langen than The may ok the moon. Hence, it - Confidence: 0.8428501486778259\n",
      "Detected Text: accelerale at a saile much mare Than the acceleralion - Confidence: 0.8634287714958191\n",
      "Detected Text: nate af the moon iowandr the eanth Fon this rearon - Confidence: 0.8300524950027466\n",
      "Detected Text: the Eailh doesn't move lowands the moon - Confidence: 0.8661238551139832\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'image_tensor_list' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 51\u001b[0m\n\u001b[1;32m     45\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     46\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: query},\n\u001b[1;32m     47\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     48\u001b[0m ]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Process inputs\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Update input parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Coding/military_int_icc/shakti-2B-041224/processing_shakti.py:239\u001b[0m, in \u001b[0;36mshaktiProcessor.__call__\u001b[0;34m(self, messages, images, videos, cut_enable, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m     num_image_shapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m image_tensor_list])\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m num_image_tokens \u001b[38;5;241m==\u001b[39m num_image_shapes, (messages, [_\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m image_tensor_list])\n\u001b[0;32m--> 239\u001b[0m image_tensor_list \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43mimage_tensor_list\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# text = ''.join([_['text'] for _ in text])\u001b[39;00m\n\u001b[1;32m    242\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_text_qwen(messages)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'image_tensor_list' referenced before assignment"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# # Load and process image\n",
    "# image = Image.new('RGB', (500, 500), color='red')\n",
    "\n",
    "# List of queries which user can enter \n",
    "queries = [\n",
    "    \"\"\"\n",
    "Give me complete corrected lines using below context:\n",
    "Detected Text: State the universal law ak gravilalion. - Confidence: 0.9311787486076355\n",
    "Detected Text: a force called he gravilalional korce. the korce acling - Confidence: 0.8632688522338867\n",
    "Detected Text: Whai do you mean by free fall? - Confidence: 0.9386471509933472\n",
    "Detected Text: tonce. Hhe maion at dhe object is said lo have free fall. - Confidence: 0.8472165465354919\n",
    "Detected Text: What do youmean by accelenalion due lo giavidu? - Confidence: 0.8162661790847778\n",
    "Detected Text: When an object kalls'Howands the graund Knom a height, - Confidence: 0.8196004033088684\n",
    "Detected Text: Velocity praduces accelenalion in ihe obiect yhis - Confidence: 0.859413206577301\n",
    "Detected Text: acceleralian in Known ar acceleralion due to gravly.Ai - Confidence: 0.8293883800506592\n",
    "Detected Text: value is gwven by g.8 ml - Confidence: 0.8043153285980225\n",
    "Detected Text: Ik the moon allnaci the eailh;why does the failh - Confidence: 0.8403022289276123\n",
    "Detected Text: nat mave dowatds Jhe moon ? - Confidence: 0.8502532839775085\n",
    "Detected Text: yhe Eaih and the moon expencences equlal gravitalionad - Confidence: 0.8557673096656799\n",
    "Detected Text: tonce txom each othen.However, the may of The Eailh - Confidence: 0.8026825189590454\n",
    "Detected Text: W much langen than The may ok the moon. Hence, it - Confidence: 0.8428501486778259\n",
    "Detected Text: accelerale at a saile much mare Than the acceleralion - Confidence: 0.8634287714958191\n",
    "Detected Text: nate af the moon iowandr the eanth Fon this rearon - Confidence: 0.8300524950027466\n",
    "Detected Text: the Eailh doesn't move lowands the moon - Confidence: 0.8661238551139832\n",
    "\"\"\"\n",
    "]\n",
    "# # Set the maximum width for the image\n",
    "# max_width = 300  # Desired maximum width\n",
    "\n",
    "# # Calculate the new height while maintaining the aspect ratio\n",
    "# aspect_ratio = image.height / image.width\n",
    "# new_width = min(image.width, max_width)  # Ensure the width doesn't exceed max_width\n",
    "# new_height = int(new_width * aspect_ratio)\n",
    "\n",
    "# # Resize the image while maintaining its aspect ratio\n",
    "# resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "# # Display the resized image in the Jupyter Notebook\n",
    "# display(resized_image)\n",
    "\n",
    "# Process each query\n",
    "for idx, query in enumerate(queries, start=1):\n",
    "    print(f\"Query {idx}: {query}\")\n",
    "    # Prepare messages\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    ]\n",
    "\n",
    "    print(\"Final text message jo model me jaayega: \", messages)\n",
    "\n",
    "    # Process inputs\n",
    "    inputs = processor(messages, images= None, videos=None)\n",
    "    inputs.to('cuda')\n",
    "\n",
    "    # Update input parameters\n",
    "    inputs.update({\n",
    "        'tokenizer': tokenizer,\n",
    "        'max_new_tokens': 100,\n",
    "        'decode_text': True,\n",
    "    })\n",
    "\n",
    "    # Generate description\n",
    "    output = model.generate(**inputs)\n",
    "\n",
    "    # Print the answer\n",
    "    print(\"Answer:\")\n",
    "    for sentence in output[0].split('.'):\n",
    "        if sentence.strip():  # Avoid empty lines caused by trailing periods\n",
    "            wrapped_text = textwrap.fill(sentence.strip(), width=100)  # Adjust line width as needed\n",
    "            print(wrapped_text)\n",
    "    print(\"\\n\" + \"-\" * 80 + \"\\n\")  # Separator between queries\n",
    "\n",
    "    # Clear the GPU memory after 5 seconds\n",
    "    # time.sleep(5)  # Wait for 5 seconds\n",
    "    # torch.cuda.empty_cache()  # Clear the CUDA cache\n",
    "\n",
    "    # Optionally, you can check memory usage after clearing the cache\n",
    "    # print(\"GPU Memory Cleared\")\n",
    "    # print(f\"Free Memory: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd26dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a201647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raw/Desktop/Coding/udit_new_military_int_icc/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/raw/Desktop/Coding/udit_new_military_int_icc/venv/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:520: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "HyperChakravyuhaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1:\n",
      "An error occurred: local variable 'image_tensor_list' referenced before assignment\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7041d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71282f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import textwrap\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer\n",
    "# from PIL import Image\n",
    "# import importlib.util\n",
    "# import sys\n",
    "# import time\n",
    "# from typing import List, Optional, Union\n",
    "\n",
    "# def setup_model(model_dir: str):\n",
    "#     \"\"\"\n",
    "#     Set up and initialize the model, tokenizer, and processor\n",
    "#     \"\"\"\n",
    "#     sys.path.append(model_dir)\n",
    "\n",
    "#     # Dynamically import configuration and model\n",
    "#     config_module_path = f\"{model_dir}/configuration_shakti.py\"\n",
    "#     model_module_path = f\"{model_dir}/modeling_shakti.py\"\n",
    "\n",
    "#     # Load configuration module\n",
    "#     spec_config = importlib.util.spec_from_file_location(\"shaktiConfig\", config_module_path)\n",
    "#     config_module = importlib.util.module_from_spec(spec_config)\n",
    "#     sys.modules[\"shaktiConfig\"] = config_module\n",
    "#     spec_config.loader.exec_module(config_module)\n",
    "\n",
    "#     # Load model module\n",
    "#     spec_model = importlib.util.spec_from_file_location(\"shaktiModel\", model_module_path)\n",
    "#     model_module = importlib.util.module_from_spec(spec_model)\n",
    "#     sys.modules[\"shaktiModel\"] = model_module\n",
    "#     spec_model.loader.exec_module(model_module)\n",
    "\n",
    "#     # Import required classes\n",
    "#     from shaktiConfig import shaktiConfig\n",
    "#     from shaktiModel import shaktiModel\n",
    "\n",
    "#     # Initialize model components\n",
    "#     config = shaktiConfig.from_pretrained(model_dir)\n",
    "#     model = shaktiModel.from_pretrained(\n",
    "#         model_dir, \n",
    "#         config=config, \n",
    "#         attn_implementation='sdpa', \n",
    "#         torch_dtype=torch.half\n",
    "#     )\n",
    "#     model.eval().cuda()\n",
    "\n",
    "#     # Initialize tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "#     # Initialize processor\n",
    "#     processor = model.init_processor(tokenizer)\n",
    "\n",
    "#     return model, tokenizer, processor\n",
    "\n",
    "# def process_inputs(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     processor,\n",
    "#     images: Optional[Union[str, List[str]]] = None,\n",
    "#     query: Optional[str] = None,\n",
    "#     max_new_tokens: int = 100,\n",
    "#     clear_gpu_memory: bool = False,\n",
    "#     gpu_clear_delay: int = 5\n",
    "# ) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     Process inputs based on different scenarios:\n",
    "#     1. Multiple images with one query\n",
    "#     2. Single image with one query\n",
    "#     3. Image(s) only (generates general description)\n",
    "#     4. Query only (generates text response)\n",
    "#     \"\"\"\n",
    "#     # Convert single image path to list\n",
    "#     if isinstance(images, str):\n",
    "#         images = [images]\n",
    "    \n",
    "#     # Process images if provided\n",
    "#     processed_images = []\n",
    "#     if images:\n",
    "#         for img_path in images:\n",
    "#             try:\n",
    "#                 img = Image.open(img_path).convert(\"RGB\")\n",
    "#                 processed_images.append(img)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "#                 continue\n",
    "    \n",
    "#     # Prepare default query if none provided\n",
    "#     if not query and processed_images:\n",
    "#         query = \"What can you see in the image(s) in detail?\"\n",
    "    \n",
    "#     # Prepare messages based on scenario\n",
    "#     messages = []\n",
    "#     if processed_images and query:\n",
    "#         messages = [\n",
    "#             {\"role\": \"user\", \"content\": f\"<|image|> {query}\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"\"}\n",
    "#         ]\n",
    "#     elif processed_images:\n",
    "#         messages = [\n",
    "#             {\"role\": \"user\", \"content\": \"<|image|> Please describe what you see.\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"\"}\n",
    "#         ]\n",
    "#     elif query:\n",
    "#         messages = [\n",
    "#             {\"role\": \"user\", \"content\": query},\n",
    "#             {\"role\": \"assistant\", \"content\": \"\"}\n",
    "#         ]\n",
    "#     else:\n",
    "#         raise ValueError(\"Either images or query must be provided\")\n",
    "\n",
    "#     # Process inputs\n",
    "#     inputs = processor(\n",
    "#         messages,\n",
    "#         images=processed_images if processed_images else None,\n",
    "#         videos=None\n",
    "#     )\n",
    "#     inputs.to('cuda')\n",
    "\n",
    "#     # Update input parameters\n",
    "#     inputs.update({\n",
    "#         'tokenizer': tokenizer,\n",
    "#         'max_new_tokens': max_new_tokens,\n",
    "#         'decode_text': True,\n",
    "#     })\n",
    "\n",
    "#     # Generate response\n",
    "#     outputs = []\n",
    "#     try:\n",
    "#         output = model.generate(**inputs)\n",
    "        \n",
    "#         # Format and store the response\n",
    "#         if isinstance(output, list):\n",
    "#             for response in output:\n",
    "#                 formatted_response = []\n",
    "#                 for sentence in response.split('.'):\n",
    "#                     if sentence.strip():\n",
    "#                         wrapped_text = textwrap.fill(sentence.strip(), width=100)\n",
    "#                         formatted_response.append(wrapped_text)\n",
    "#                 outputs.append('\\n'.join(formatted_response))\n",
    "#         else:\n",
    "#             formatted_response = []\n",
    "#             for sentence in output.split('.'):\n",
    "#                 if sentence.strip():\n",
    "#                     wrapped_text = textwrap.fill(sentence.strip(), width=100)\n",
    "#                     formatted_response.append(wrapped_text)\n",
    "#             outputs.append('\\n'.join(formatted_response))\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error generating response: {str(e)}\")\n",
    "#         outputs.append(f\"Error: {str(e)}\")\n",
    "\n",
    "#     # Clear GPU memory if requested\n",
    "#     if clear_gpu_memory:\n",
    "#         time.sleep(gpu_clear_delay)\n",
    "#         torch.cuda.empty_cache()\n",
    "#         print(\"GPU Memory Cleared\")\n",
    "#         print(f\"Free Memory: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "#     return outputs\n",
    "\n",
    "# def display_image(image_path: str, max_width: int = 300):\n",
    "#     \"\"\"\n",
    "#     Display image with specified maximum width while maintaining aspect ratio\n",
    "#     \"\"\"\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     aspect_ratio = image.height / image.width\n",
    "#     new_width = min(image.width, max_width)\n",
    "#     new_height = int(new_width * aspect_ratio)\n",
    "#     resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "#     display(resized_image)\n",
    "\n",
    "# def main():\n",
    "#     # Initialize model and components\n",
    "#     model_dir = '/home/raw/Desktop/Coding/military_int_icc/shakti-2B-041224'\n",
    "#     model, tokenizer, processor = setup_model(model_dir)\n",
    "\n",
    "#     # Example usage for different scenarios\n",
    "#     try:\n",
    "#         # Scenario 1: Multiple images with one query\n",
    "#         images = ['/home/raw/Downloads/11.jpg', '/home/raw/Downloads/12.jpg']\n",
    "#         query = \"What objects can you identify in these images?\"\n",
    "#         print(\"\\nProcessing multiple images with query...\")\n",
    "#         for img in images:\n",
    "#             display_image(img)\n",
    "#         responses = process_inputs(model, tokenizer, processor, images=images, query=query)\n",
    "#         for i, response in enumerate(responses, 1):\n",
    "#             print(f\"\\nResponse {i}:\")\n",
    "#             print(response)\n",
    "#             print(\"-\" * 80)\n",
    "\n",
    "#         # Scenario 2: Single image with query\n",
    "#         print(\"\\nProcessing single image with query...\")\n",
    "#         display_image(images[0])\n",
    "#         response = process_inputs(\n",
    "#             model, tokenizer, processor,\n",
    "#             images=images[0],\n",
    "#             query=\"Describe this image in detail.\"\n",
    "#         )\n",
    "#         print(\"\\nResponse:\")\n",
    "#         print(response[0])\n",
    "#         print(\"-\" * 80)\n",
    "\n",
    "#         # Scenario 3: Image only\n",
    "#         print(\"\\nProcessing image only...\")\n",
    "#         display_image(images[0])\n",
    "#         response = process_inputs(model, tokenizer, processor, images=images[0])\n",
    "#         print(\"\\nResponse:\")\n",
    "#         print(response[0])\n",
    "#         print(\"-\" * 80)\n",
    "\n",
    "#         # Scenario 4: Query only\n",
    "#         print(\"\\nProcessing query only...\")\n",
    "#         response = process_inputs(\n",
    "#             model, tokenizer, processor,\n",
    "#             query=\"What are the potential implications of this situation?\"\n",
    "#         )\n",
    "#         print(\"\\nResponse:\")\n",
    "#         print(response[0])\n",
    "#         print(\"-\" * 80)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80149df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbcef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import textwrap\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer\n",
    "# from PIL import Image\n",
    "# import importlib.util\n",
    "# import sys\n",
    "# import os  # Import os module to manage processes\n",
    "# import time  # Import time module to introduce delay\n",
    "\n",
    "# model_dir = '/home/raw/Desktop/Coding/military_int_icc/shakti-2B-041224'\n",
    "# sys.path.append(model_dir)\n",
    "\n",
    "# # Dynamically import the configuration and model\n",
    "# config_module_path = f\"{model_dir}/configuration_shakti.py\"\n",
    "# model_module_path = f\"{model_dir}/modeling_shakti.py\"\n",
    "\n",
    "# # Load the configuration module dynamically\n",
    "# spec_config = importlib.util.spec_from_file_location(\"shaktiConfig\", config_module_path)\n",
    "# config_module = importlib.util.module_from_spec(spec_config)\n",
    "# sys.modules[\"shaktiConfig\"] = config_module\n",
    "# spec_config.loader.exec_module(config_module)\n",
    "\n",
    "# # Load the model module dynamically\n",
    "# spec_model = importlib.util.spec_from_file_location(\"shaktiModel\", model_module_path)\n",
    "# model_module = importlib.util.module_from_spec(spec_model)\n",
    "# sys.modules[\"shaktiModel\"] = model_module\n",
    "# spec_model.loader.exec_module(model_module)\n",
    "\n",
    "# # Now you can use the classes from the dynamically loaded modules\n",
    "# from shaktiConfig import shaktiConfig\n",
    "# from shaktiModel import shaktiModel\n",
    "\n",
    "# # Load the custom model configuration\n",
    "# config = shaktiConfig.from_pretrained(model_dir)\n",
    "\n",
    "# # Load the custom model using the configuration\n",
    "# model = shaktiModel.from_pretrained(model_dir, config=config, attn_implementation='sdpa', torch_dtype=torch.half)\n",
    "# model.eval().cuda()\n",
    "\n",
    "# # Initialize the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# # Initialize the processor (if custom processor function is provided in the model)\n",
    "# processor = model.init_processor(tokenizer)\n",
    "\n",
    "# # Load and process image\n",
    "# image = Image.open('/home/raw/Downloads/11.jpg').convert(\"RGB\")\n",
    "\n",
    "# # List of queries which user can enter \n",
    "# queries = [\n",
    "#     \"What can you see in the image in detail?\",\n",
    "#     \"How can we prevent it?\",\n",
    "#     \"Give me a breif\",\n",
    "# ]\n",
    "\n",
    "# # Set the maximum width for the image\n",
    "# max_width = 300  # Desired maximum width\n",
    "\n",
    "# # Calculate the new height while maintaining the aspect ratio\n",
    "# aspect_ratio = image.height / image.width\n",
    "# new_width = min(image.width, max_width)  # Ensure the width doesn't exceed max_width\n",
    "# new_height = int(new_width * aspect_ratio)\n",
    "\n",
    "# # Resize the image while maintaining its aspect ratio\n",
    "# resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "# # Display the resized image in the Jupyter Notebook\n",
    "# display(resized_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Process each query\n",
    "# for idx, query in enumerate(queries, start=1):\n",
    "#     print(f\"Query {idx}: {query}\")\n",
    "#     # Prepare messages\n",
    "#     messages = [\n",
    "#         {\"role\": \"user\", \"content\": f\"\"\"<|image|>\n",
    "# {query}\"\"\"},\n",
    "#         {\"role\": \"assistant\", \"content\": \"\"}\n",
    "#     ]\n",
    "\n",
    "#     # Process inputs\n",
    "#     inputs = processor(messages, images=[image], videos=None)\n",
    "#     inputs.to('cuda')\n",
    "\n",
    "#     # Update input parameters\n",
    "#     inputs.update({\n",
    "#         'tokenizer': tokenizer,\n",
    "#         'max_new_tokens': 500,\n",
    "#         'decode_text': True,\n",
    "#     })\n",
    "\n",
    "#     # Generate description\n",
    "#     output = model.generate(**inputs)\n",
    "\n",
    "#     # Clear the GPU memory after 5 seconds\n",
    "#     time.sleep(5)  # Wait for 5 seconds\n",
    "#     torch.cuda.empty_cache()  # Clear the CUDA cache\n",
    "\n",
    "#     # # Optionally, kill the process to free GPU memory\n",
    "#     # current_pid = os.getpid()  # Get the process ID of the current script\n",
    "#     # print(f\"Killing process with PID: {current_pid}\")\n",
    "#     # os.system(f\"kill -9 {current_pid}\")  # Force kill the current process\n",
    "#     # break  # Exit after killing the process to avoid unnecessary iterations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model_loader import model, tokenizer, processor  # Importing from model_loader.py\n",
    "# import asyncio\n",
    "# from fastapi import FastAPI, UploadFile, Form, HTTPException\n",
    "# from fastapi.responses import JSONResponse\n",
    "# from fastapi.middleware.cors import CORSMiddleware\n",
    "# from pydantic import BaseModel\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer\n",
    "# import importlib.util\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# # Initialize FastAPI app\n",
    "# app = FastAPI()\n",
    "\n",
    "# # Add CORS middleware\n",
    "# # app.add_middleware(\n",
    "# #     CORSMiddleware,\n",
    "# #     allow_origins=[\"*\"],  # Change \"*\" to specific domains for production\n",
    "# #     allow_credentials=True,\n",
    "# #     allow_methods=[\"*\"],\n",
    "# #     allow_headers=[\"*\"],\n",
    "# # )\n",
    "\n",
    "\n",
    "# @app.post(\"/analyze/\")\n",
    "# async def analyze(query: str = Form(...), image: UploadFile = None):\n",
    "#     try:\n",
    "#         # Process the image if provided\n",
    "#         img = None\n",
    "#         if image:\n",
    "#             img = Image.open(image.file).convert(\"RGB\")\n",
    "#             print(\"Image loaded successfully:\", img.size)\n",
    "\n",
    "#         # Prepare messages\n",
    "#         messages = [\n",
    "#             {\"role\": \"user\", \"content\": f\"\"\"<|image|>\n",
    "# {query}\"\"\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"\"}\n",
    "#         ]\n",
    "\n",
    "#         print(\"Query:\", query)\n",
    "#         print(\"Image:\", img)\n",
    "\n",
    "#         # Process inputs\n",
    "#         inputs = processor(messages, images=[img] if img else None, videos=None)\n",
    "#         inputs.to('cuda')\n",
    "#         inputs.update({\n",
    "#             'tokenizer': tokenizer,\n",
    "#             'max_new_tokens': 500,\n",
    "#             'decode_text': True,\n",
    "#         })\n",
    "\n",
    "#         # Generate the response\n",
    "#         print(\"Output generation is started. \")\n",
    "#         output = model.generate(**inputs)\n",
    "\n",
    "#         print(\"Output generation is ended. \")\n",
    "#         print(output[0])\n",
    "\n",
    "#         # Send the output\n",
    "#         return JSONResponse(content={\"response\": output[0]})\n",
    "#     except Exception as e:\n",
    "#         print(\"Error:\", str(e))\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70da2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56055204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78c5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339fa30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94020bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04339eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85234e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4fd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda15181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f394269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd18e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404c7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37bdfbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d20f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
